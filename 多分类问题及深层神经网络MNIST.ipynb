{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2019)\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=2)\n",
    "plt.tight_layout(w_pad=-2.0, h_pad = -8.0)\n",
    "images, labels = train_set.next_batch(12, shuffle=False)\n",
    "for ind,(image,label) in enumerate(zip(images, labels)):\n",
    "    image = image.reshape((28,28))\n",
    "    label = label.argmax()\n",
    "    row = ind//6\n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image,cmap='gray')\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义深度网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(layer_input, output_depth, scope = \"hidden_layer\", reuse=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                            shape=(input_depth, output_depth),name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name=\"bias\")\n",
    "        net = tf.matmul(layer_input,w)+b\n",
    "        \n",
    "        return net\n",
    "\n",
    "def DNN(x, output_depths, scope='DNN', reuse=None):\n",
    "    net=x\n",
    "    for i , output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net,output_depth,scope='layer%d' % i, reuse=reuse)\n",
    "        net = tf.nn.relu(net)\n",
    "    net = hidden_layer(net,10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(shape=(None,784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None,10), dtype=tf.int64)\n",
    "#4层\n",
    "dnn = DNN(input_ph, [400, 200, 100])\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)),\n",
    "                             dtype=tf.float32))\n",
    "lr = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1000: train_loss: 0.253653 train_acc: 0.921875  test_loss: 0.336481 test_acc: 0.906250\n",
      "STEP2000: train_loss: 0.248945 train_acc: 0.937500  test_loss: 0.306756 test_acc: 0.921875\n",
      "STEP3000: train_loss: 0.103879 train_acc: 1.000000  test_loss: 0.073909 test_acc: 1.000000\n",
      "STEP4000: train_loss: 0.120947 train_acc: 0.937500  test_loss: 0.258381 test_acc: 0.890625\n",
      "STEP5000: train_loss: 0.161666 train_acc: 0.953125  test_loss: 0.301629 test_acc: 0.875000\n",
      "STEP6000: train_loss: 0.104920 train_acc: 0.968750  test_loss: 0.063372 test_acc: 0.984375\n",
      "STEP7000: train_loss: 0.110567 train_acc: 0.984375  test_loss: 0.092887 test_acc: 0.968750\n",
      "STEP8000: train_loss: 0.072268 train_acc: 1.000000  test_loss: 0.074406 test_acc: 0.984375\n",
      "STEP9000: train_loss: 0.166160 train_acc: 0.937500  test_loss: 0.084829 test_acc: 0.968750\n",
      "STEP10000: train_loss: 0.195742 train_acc: 0.937500  test_loss: 0.104386 test_acc: 0.937500\n",
      "STEP11000: train_loss: 0.034972 train_acc: 1.000000  test_loss: 0.193936 test_acc: 0.937500\n",
      "STEP12000: train_loss: 0.166773 train_acc: 0.968750  test_loss: 0.039260 test_acc: 1.000000\n",
      "STEP13000: train_loss: 0.112020 train_acc: 0.953125  test_loss: 0.066516 test_acc: 0.968750\n",
      "STEP14000: train_loss: 0.027088 train_acc: 1.000000  test_loss: 0.046660 test_acc: 0.984375\n",
      "STEP15000: train_loss: 0.033514 train_acc: 0.984375  test_loss: 0.047871 test_acc: 0.984375\n",
      "STEP16000: train_loss: 0.034640 train_acc: 1.000000  test_loss: 0.105712 test_acc: 0.968750\n",
      "STEP17000: train_loss: 0.032083 train_acc: 1.000000  test_loss: 0.251949 test_acc: 0.921875\n",
      "STEP18000: train_loss: 0.049569 train_acc: 0.984375  test_loss: 0.146536 test_acc: 0.937500\n",
      "STEP19000: train_loss: 0.100732 train_acc: 0.968750  test_loss: 0.040817 test_acc: 0.984375\n",
      "STEP20000: train_loss: 0.023226 train_acc: 1.000000  test_loss: 0.036058 test_acc: 0.984375\n",
      "Train Done !\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e%1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        loss_train, acc_train = sess.run([loss,acc], feed_dict={input_ph:images, label_ph: labels})\n",
    "        loss_test, acc_test = sess.run([loss,acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        print('STEP{}: train_loss: {:.6f} train_acc: {:.6f}  test_loss: {:.6f} test_acc: {:.6f}'.format(e+1, loss_train,acc_train,loss_test,acc_test))\n",
    "\n",
    "print(\"Train Done !\")\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.054127\n",
      "Train accuracy: 0.984582\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss,acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "print(\"Train loss: {:.6f}\".format(np.array(train_loss).mean()))\n",
    "print(\"Train accuracy: {:.6f}\".format(np.array(train_acc).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.091533\n",
      "Test accuracy: 0.971100\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "    \n",
    "print(\"Test loss: {:.6f}\".format(np.array(test_loss).mean()))\n",
    "print(\"Test accuracy: {:.6f}\".format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_sum = tf.summary.scalar('loss',loss)\n",
    "#w_hist = tf.summary.histogram('w_hist',w)\n",
    "#iamge_sum = tf.summary.image('iamge', image)\n",
    "#audio_sum = tf.summary.audio('audio', audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(shape=(None,784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None,10),dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev= tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(x, output_dim, scope = 'hidden_layer', act = tf.nn.relu, reuse=None):\n",
    "    input_dim = x.get_shape()[-1]\n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            weight = weight_variable(input_dim, output_dim)\n",
    "            variable_summaries(weight)\n",
    "        with tf.name_scope('bias'):\n",
    "            bias = bias_variable(output_dim)\n",
    "            variable_summaries(bias)\n",
    "        with name_scope('linear'):\n",
    "            preact = tf.matmul(x,weight)+bias\n",
    "            tf.summary.histogram('pre_activation',preact)\n",
    "            \n",
    "        output = act(preact)\n",
    "        tf.summary.histogram('output', output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
