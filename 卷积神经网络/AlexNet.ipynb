{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "图像识别一直是机器学习中非常重要的一个研究内容, 2010年开始举办的[ILSVRC](http://image-net.org/challenges/LSVRC/)比赛更是吸引了无数的团队. 这个比赛基于一个百万量级的图片数据集, 提出一个图像1000分类的挑战. 前两年在比赛中脱颖而出的都是经过人工挑选特征, 再通过`SVM`或者`随机森林`这样在过去十几年中非常成熟的机器学习方法进行分类的算法. \n",
    "\n",
    "在2012年, 由 [Alex Krizhevsky](https://www.cs.toronto.edu/~kriz/), [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/)提出了一种使用卷积神经网络的方法, 以 [0.85](http://image-net.org/challenges/LSVRC/2012/results.html#abstract) 的`top-5`正确率一举获得当年分类比赛的冠军, 超越使用传统方法的第二名10个百分点, 震惊了当时的学术界, 从此开启了人工智能领域的新篇章.\n",
    "\n",
    "这次的课程我们就来复现一次`AlexNet`, 首先来看它的网络结构\n",
    "\n",
    "<img src=\"https://image.ibb.co/mP9C0x/alexnet_arch.png\">\n",
    "\n",
    "可以看出`AlexNet`就是几个卷积池化堆叠后连接几个全连接层, 下面就让我们来尝试仿照这个结构来解决[cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)分类问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CIFAR10`\n",
    "\n",
    "[![cifar10](https://image.ibb.co/n885Lx/cifar10.png)](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "这是一个包含60000张$32\\times32$图片的数据库, 包含50000张训练集和10000张测试集, 在这里我们提供一个脚本帮助我们读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据\n",
    "#data_dir = 'cifar10_data'\n",
    "#cifar10_input.maybe_download(data_dir='cifar10_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集\n",
    "# 在使用随机梯度下降法的时候, 训练集要求打乱样本\n",
    "#train_imgs, train_labels = cifar10_input.inputs(eval_data=False, \n",
    "#                                                data_dir='%s/cifar-10-batches-bin/' % data_dir, \n",
    "#                                                batch_size=batch_size, \n",
    "#                                                shuffle=True)\n",
    "\n",
    "# 获取测试集\n",
    "# 测试集不需要打乱样本\n",
    "#val_imgs, val_labels = cifar10_input.inputs(eval_data=True, \n",
    "#                                            data_dir='%s/cifar-10-batches-bin/' % data_dir, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "import cifar10_input\n",
    "\n",
    "data_dir = \"../cifar_data/cifar-10-batches-bin/\"\n",
    "batch_size = 64\n",
    "\n",
    "train_imgs, train_labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "\n",
    "val_images, val_labels = cifar10_input.inputs(eval_data=True,data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                               batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN # 训练样本的个数\n",
    "val_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL       # 测试样本的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 像之前一样, 我们构造几个生成变量的函数\n",
    "def variable_weight(shape, stddev=5e-2):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(shape=shape, initializer=init, name='weight')\n",
    "\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(shape=shape, initializer=init, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的课程大家已经见过如何使用卷积和池化了, 但是由于`tensorflow`提供的接口是底层的, 为了方便, 我们写一个上层的接口来调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, ksize, out_depth, strides, padding='SAME', act=tf.nn.relu, scope='conv_layer', reuse=None):\n",
    "    \"\"\"构造一个卷积层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        ksize: 卷积核的大小, 一个长度为2的`list`, 例如[3, 3]\n",
    "        output_depth: 卷积核的个数\n",
    "        strides: 卷积核移动的步长, 一个长度为2的`list`, 例如[2, 2]\n",
    "        padding: 卷积核的补0策略\n",
    "        act: 完成卷积后的激活函数, 默认是`tf.nn.relu`\n",
    "        scope: 这一层的名称(可选)\n",
    "        reuse: 是否复用\n",
    "    \n",
    "    Return:\n",
    "        out: 卷积层的结果\n",
    "    \"\"\"\n",
    "    # 这里默认数据是NHWC输入的\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 先构造卷积核\n",
    "        shape = ksize + [in_depth, out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(shape)\n",
    "            \n",
    "        strides = [1, strides[0], strides[1], 1]\n",
    "        # 生成卷积\n",
    "        conv = tf.nn.conv2d(x, kernel, strides, padding, name='conv')\n",
    "        \n",
    "        # 构造偏置\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "            \n",
    "        # 和偏置相加\n",
    "        preact = tf.nn.bias_add(conv, bias)\n",
    "        \n",
    "        # 添加激活层\n",
    "        out = act(preact)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(x, ksize, strides, padding='SAME', name='pool_layer'):\n",
    "    \"\"\"构造一个最大值池化层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        ksize: pooling核的大小, 一个长度为2的`list`, 例如[3, 3]\n",
    "        strides: pooling核移动的步长, 一个长度为2的`list`, 例如[2, 2]\n",
    "        padding: pooling的补0策略\n",
    "        name: 这一层的名称(可选)\n",
    "    \n",
    "    Return:\n",
    "        pooling层的结果\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, [1, ksize[0], ksize[1], 1], [1, strides[0], strides[1], 1], padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    \"\"\"构造一个全连接层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        out_depth: 输出向量的维数\n",
    "        act: 激活函数, 默认是`tf.nn.relu`\n",
    "        scope: 名称域, 默认是`fully_connect`\n",
    "        reuse: 是否需要重用\n",
    "    \"\"\"\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    # 构造全连接层的参数\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 构造权重\n",
    "        with tf.variable_scope('weight'):\n",
    "            weight = variable_weight([in_depth, out_depth])\n",
    "            \n",
    "        # 构造偏置项\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        \n",
    "        # 一个线性函数\n",
    "        fc = tf.nn.bias_add(tf.matmul(x, weight), bias, name='fc')\n",
    "        \n",
    "        # 激活函数作用\n",
    "        out = act(fc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面这些上层函数之后, 我们就可以轻松构建`AlexNet`了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(inputs, reuse=None):\n",
    "    \"\"\"构建 Alexnet 的前向传播\n",
    "    Args:\n",
    "        inpus: 输入\n",
    "        reuse: 是否需要重用\n",
    "        \n",
    "    Return:\n",
    "        net: alexnet的结果\n",
    "    \"\"\"\n",
    "    # 首先我们声明一个变量域`AlexNet`\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        # 第一层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(inputs, [5, 5], 64, [1, 1], padding='VALID', scope='conv1')\n",
    "        \n",
    "        # 第二层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool1')\n",
    "        \n",
    "        # 第三层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(net, [5, 5], 64, [1, 1], scope='conv2')\n",
    "        \n",
    "        # 第四层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool2')\n",
    "        #print(net.get_shape())\n",
    "        # 将矩阵拉长成向量\n",
    "        net = tf.reshape(net, [-1, 4*4*64])\n",
    "        \n",
    "        # 第五层是全连接层, 输出个数为384\n",
    "        net = fc(net, 384, scope='fc3')\n",
    "        \n",
    "        # 第六层是全连接层, 输出个数为192\n",
    "        net = fc(net, 192, scope='fc4')\n",
    "        \n",
    "        # 第七层是全连接层, 输出个数为10, 注意这里不要使用激活函数\n",
    "        net = fc(net, 10, scope='fc5', act=tf.identity)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 alexnet 构建训练和测试的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = alexnet(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意当再次调用 alexnet 函数时, 如果要使用之前调用时产生的变量值, **必须**要重用变量域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out = alexnet(val_images, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "这里真实的 labels 不是一个 one_hot 型的向量, 而是一个数值, 因此我们使用 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义正确率`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('train'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造训练`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们已经将训练过程封装好了, 感兴趣的同学可以进入`train.py`查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 20000步的训练后, `AlexNet`在训练集和测试集分别达到了`0.97`和`0.72`的准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
