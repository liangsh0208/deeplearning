{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10_input\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "images_trains, labels_trains = cifar10_input.distorted_inputs(data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                                              batch_size=batch_size)\n",
    "\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True,data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                               batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_weight(shape, stddev=5e-2):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='weight')\n",
    "\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='bias')\n",
    "\n",
    "def conv(x, ksize, out_depth, strides, padding='SAME', act=tf.nn.relu, scope='conv_layer', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = ksize+[in_depth, out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(shape=shape)\n",
    "        strides = [1, strides[0], strides[1],1]\n",
    "        conv = tf.nn.conv2d(x, kernel, strides, padding, name='conv')\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        preact = tf.nn.bias_add(conv,bias)\n",
    "        out = act(preact)\n",
    "        return out\n",
    "\n",
    "def max_pool(x, ksize, strides, padding='SAME', name='pool_layer'):\n",
    "    return tf.nn.max_pool(x,[1, ksize[0], ksize[1],1],[1,strides[0], strides[1],1], padding, name=name)\n",
    "\n",
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with tf.variable_scope('weight'):\n",
    "            weitht = variable_weight([in_depth, out_depth])\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias= variable_bias([out_depth])\n",
    "        fc = tf.nn.bias_add(tf.matmul(x,weitht),bias, name='fc')\n",
    "        out = act(fc)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#真正意义上的深层网络结构， imageNet2014亚军\n",
    "def vgg_block(inputs, num_convs,out_depth, scope=\"vgg_block\", reuse=None ):\n",
    "    #一个block有num_convs个卷积层和一个最大池化层组成\n",
    "    input_depth = inputs.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i in range(num_convs):\n",
    "            net = conv(net, ksize=[3,3], out_depth=out_depth, strides=[1,1], padding='SAME', \n",
    "                       scope='conv%d' % i,reuse=reuse)\n",
    "        net = max_pool(net, ksize=[2,2], strides=[2,2], name='pool')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(inputs, num_convs, out_depths, scope = 'vgg_stack', reuse=None):\n",
    "    #输入， 每个block卷积层的个数； 每个block卷积核的个数\n",
    "    with tf.variable_scope(scope,reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i , (n,d) in enumerate(zip(num_convs, out_depths)):\n",
    "            net = vgg_block(net, n, d, scope='block%d' % i)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入全链接搭建vgg\n",
    "def vgg(inputs, num_convs,out_depths, num_outputs, scope='vgg', reuse=None):\n",
    "    #经过stack后连接两个全连接层\n",
    "    net = inputs\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = vgg_stack(net, num_convs=num_convs, out_depths=out_depths)\n",
    "        with tf.variable_scope('classification'):\n",
    "            net = tf.reshape(net,(batch_size,-1))\n",
    "            net = fc(net, 100, scope='fc1')\n",
    "            net = fc(net, num_outputs, act=tf.identity, scope='classification')\n",
    "            \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_holder = tf.placeholder(shape=[batch_size,24,24,3], dtype=tf.float32)\n",
    "label_holder = tf.placeholder(shape=[batch_size], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = vgg(image_holder, (1,1,2,2,2), (64,128,256,512,512), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=train_out, \n",
    "                                                                       labels=label_holder, scope='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('accuracy'):\n",
    "    with tf.name_scope('tran'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32),\n",
    "                                                    label_holder),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "#train_op = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(train_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-4889ccde473e>:1: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 139981188208384)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980551530240)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980543137536)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980534744832)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980526352128)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980517959424)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980164822784)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980156430080)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980148037376)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980139644672)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980131251968)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980122859264)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139980114466560)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139979963496192)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139979955103488)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139979946710784)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139979938318080)>,\n",
       " <Thread(QueueRunnerThread-input/input_producer-input/input_producer/input_producer_EnqueueMany, started daemon 139979929925376)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139980509566720)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979921532672)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979913139968)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979829278464)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979820885760)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979812493056)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979804100352)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979795707648)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979787314944)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979778922240)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979770529536)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979762136832)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979753744128)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979745351424)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979736958720)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139979728566016)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1  train loss 3.398097 train acc: 0.140625\n",
      "Step11  train loss 2.321986 train acc: 0.093750\n",
      "Step21  train loss 2.289397 train acc: 0.109375\n",
      "Step31  train loss 2.440855 train acc: 0.062500\n",
      "Step41  train loss 2.200521 train acc: 0.281250\n",
      "Step51  train loss 2.125059 train acc: 0.125000\n",
      "Step61  train loss 2.129168 train acc: 0.156250\n",
      "Step71  train loss 2.086580 train acc: 0.171875\n",
      "Step81  train loss 2.112401 train acc: 0.187500\n",
      "Step91  train loss 2.278561 train acc: 0.125000\n",
      "Step101  train loss 2.164048 train acc: 0.125000\n",
      "Step111  train loss 2.117632 train acc: 0.187500\n",
      "Step121  train loss 1.972025 train acc: 0.218750\n",
      "Step131  train loss 2.000384 train acc: 0.234375\n",
      "Step141  train loss 1.958821 train acc: 0.281250\n",
      "Step151  train loss 1.947047 train acc: 0.234375\n",
      "Step161  train loss 1.846113 train acc: 0.296875\n",
      "Step171  train loss 2.079134 train acc: 0.187500\n",
      "Step181  train loss 1.882857 train acc: 0.265625\n",
      "Step191  train loss 2.044087 train acc: 0.234375\n",
      "Step201  train loss 1.895617 train acc: 0.203125\n",
      "Step211  train loss 1.865105 train acc: 0.312500\n",
      "Step221  train loss 1.872354 train acc: 0.187500\n",
      "Step231  train loss 1.850234 train acc: 0.296875\n",
      "Step241  train loss 1.915291 train acc: 0.328125\n",
      "Step251  train loss 1.848357 train acc: 0.296875\n",
      "Step261  train loss 1.903214 train acc: 0.265625\n",
      "Step271  train loss 1.997222 train acc: 0.281250\n",
      "Step281  train loss 1.809744 train acc: 0.312500\n",
      "Step291  train loss 1.642235 train acc: 0.312500\n",
      "Step301  train loss 1.809805 train acc: 0.265625\n",
      "Step311  train loss 1.880319 train acc: 0.234375\n",
      "Step321  train loss 1.849465 train acc: 0.281250\n",
      "Step331  train loss 1.846582 train acc: 0.281250\n",
      "Step341  train loss 2.000222 train acc: 0.250000\n",
      "Step351  train loss 1.974437 train acc: 0.296875\n",
      "Step361  train loss 1.901072 train acc: 0.343750\n",
      "Step371  train loss 1.577684 train acc: 0.390625\n",
      "Step381  train loss 1.627232 train acc: 0.328125\n",
      "Step391  train loss 1.587718 train acc: 0.390625\n",
      "Step401  train loss 1.687835 train acc: 0.359375\n",
      "Step411  train loss 1.860088 train acc: 0.328125\n",
      "Step421  train loss 2.014418 train acc: 0.203125\n",
      "Step431  train loss 1.958270 train acc: 0.375000\n",
      "Step441  train loss 1.646634 train acc: 0.359375\n",
      "Step451  train loss 1.618564 train acc: 0.406250\n",
      "Step461  train loss 1.714245 train acc: 0.343750\n",
      "Step471  train loss 1.659240 train acc: 0.421875\n",
      "Step481  train loss 1.619058 train acc: 0.343750\n",
      "Step491  train loss 1.740986 train acc: 0.343750\n",
      "Step501  train loss 1.793330 train acc: 0.390625\n",
      "Step511  train loss 1.557117 train acc: 0.515625\n",
      "Step521  train loss 1.685051 train acc: 0.390625\n",
      "Step531  train loss 1.593677 train acc: 0.312500\n",
      "Step541  train loss 1.710526 train acc: 0.343750\n",
      "Step551  train loss 1.779262 train acc: 0.343750\n",
      "Step561  train loss 1.648645 train acc: 0.484375\n",
      "Step571  train loss 1.517558 train acc: 0.468750\n",
      "Step581  train loss 1.422424 train acc: 0.515625\n",
      "Step591  train loss 1.178604 train acc: 0.609375\n",
      "Step601  train loss 1.602347 train acc: 0.468750\n",
      "Step611  train loss 1.751163 train acc: 0.390625\n",
      "Step621  train loss 1.463915 train acc: 0.500000\n",
      "Step631  train loss 1.676929 train acc: 0.421875\n",
      "Step641  train loss 1.801981 train acc: 0.343750\n",
      "Step651  train loss 1.527310 train acc: 0.437500\n",
      "Step661  train loss 1.665435 train acc: 0.437500\n",
      "Step671  train loss 1.651484 train acc: 0.343750\n",
      "Step681  train loss 1.556021 train acc: 0.375000\n",
      "Step691  train loss 1.634918 train acc: 0.421875\n",
      "Step701  train loss 1.559113 train acc: 0.390625\n",
      "Step711  train loss 1.688287 train acc: 0.421875\n",
      "Step721  train loss 1.686197 train acc: 0.343750\n",
      "Step731  train loss 1.818422 train acc: 0.359375\n",
      "Step741  train loss 1.510716 train acc: 0.515625\n",
      "Step751  train loss 1.597763 train acc: 0.343750\n",
      "Step761  train loss 1.760894 train acc: 0.406250\n",
      "Step771  train loss 1.544654 train acc: 0.453125\n",
      "Step781  train loss 1.364387 train acc: 0.515625\n",
      "Step791  train loss 1.461402 train acc: 0.515625\n",
      "Step801  train loss 1.845491 train acc: 0.359375\n",
      "Step811  train loss 1.667701 train acc: 0.359375\n",
      "Step821  train loss 1.487337 train acc: 0.406250\n",
      "Step831  train loss 1.197997 train acc: 0.546875\n",
      "Step841  train loss 1.666680 train acc: 0.406250\n",
      "Step851  train loss 1.352216 train acc: 0.453125\n",
      "Step861  train loss 1.474124 train acc: 0.453125\n",
      "Step871  train loss 1.525010 train acc: 0.484375\n",
      "Step881  train loss 1.306938 train acc: 0.562500\n",
      "Step891  train loss 1.540750 train acc: 0.375000\n",
      "Step901  train loss 1.327631 train acc: 0.515625\n",
      "Step911  train loss 1.256409 train acc: 0.546875\n",
      "Step921  train loss 1.366641 train acc: 0.500000\n",
      "Step931  train loss 1.802118 train acc: 0.265625\n",
      "Step941  train loss 1.589958 train acc: 0.453125\n",
      "Step951  train loss 1.332827 train acc: 0.562500\n",
      "Step961  train loss 1.304323 train acc: 0.484375\n",
      "Step971  train loss 1.422622 train acc: 0.531250\n",
      "Step981  train loss 1.687233 train acc: 0.421875\n",
      "Step991  train loss 1.382982 train acc: 0.562500\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    image_bath, label_batch = sess.run([images_trains, labels_trains])\n",
    "    _, loss1, acc1 = sess.run([train_op,train_loss,train_acc], feed_dict={image_holder: image_bath, label_holder: label_batch})\n",
    "    if i % 10 ==0:\n",
    "        print(\"Step%d  train loss %.6f train acc: %.6f\" % (i+1, loss1, acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
