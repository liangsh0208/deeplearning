{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet 也是基于resnet跨层连接的思想。 cvpr2017 best paper. \n",
    "\n",
    "resnet 是跨层求和， DenseNet 是跨层将特征在通道维度进行拼接。\n",
    "\n",
    "因为是在通道维度进行特征拼接，多以底层的输出会保留进入所有后边的层， 能够更好的保证梯度的传播，同时能使用低维的特征和高维的特征进行联合训练，能够得到更好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10_input\n",
    "batch_size = 64\n",
    "images_trains, labels_trains = cifar10_input.distorted_inputs(data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                                              batch_size=batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True,data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的卷积单元\n",
    "def bn_relu_conv(x, out_depth, scope='dense_basic_conv', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net = slim.batch_norm(x, activation_fn=None, scope='bn')\n",
    "        net = slim.nn.relu(net, name='activation')\n",
    "        net = slim.conv2d(net, out_depth,3, activation_fn=None, normalizer_fn=None,\n",
    "                          biases_regularizer=None, scope='conv')\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#densenet 基本单元\n",
    "def dense_block(x, growth_rate, num_layers, scope='dense_block', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net =x\n",
    "        for i in range(num_layers):\n",
    "            out = bn_relu_conv(net, growth_rate, scope='block%d' % i)\n",
    "            net = tf.concat([net, out], axis=-1)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(x, out_depth, scope='transition', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net = slim.batch_norm(x, activation_fn=None, scope='bn')\n",
    "        net = tf.nn.relu(net, name='activation')\n",
    "        net = slim.conv2d(net, out_depth, 1, activation_fn=None, normalizer_fn=None, \n",
    "                          biases_regularizer=None, scope='conv')\n",
    "        net = slim.avg_pool2d(net, 2,2, scope='avg_pool')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(x, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16], is_training=None, \n",
    "             scope='densenet', reuse=None, verbose=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "            if verbose:\n",
    "                print('input: {}'.format(x.shape))\n",
    "            \n",
    "            with tf.variable_scope('block0'):\n",
    "                net = slim.conv2d(x, 64, [7,7], stride=2, normalizer_fn=None, activation_fn=None,\n",
    "                                  scope='conv_7x7')\n",
    "                net = tf.nn.relu(net, name='activation')\n",
    "                net = slim.max_pool2d(net, [3,3], stride=2, scope='max_pool')\n",
    "                if verbose:\n",
    "                    print('block0: {}'.format(net.shape))\n",
    "            #循环构建block transition\n",
    "            for i, num_layers in enumerate(block_layers):\n",
    "                with tf.variable_scope('block%d' % (i+1)):\n",
    "                    net = dense_block(net, growth_rate, num_layers)\n",
    "                    if i != len(block_layers)-1:\n",
    "                        current_depth = net.get_shape().as_list()[-1]\n",
    "                        net = transition(net, current_depth//2)\n",
    "                if verbose:\n",
    "                    print('block{}: {}'.format(i+1, net.shape))\n",
    "                \n",
    "                with tf.variable_scope('block%d' % (len(block_layers)+1)):\n",
    "                    net = slim.batch_norm(net, activation_fn=None, scope='bn')\n",
    "                    net = tf.nn.relu(net,name='activation')\n",
    "                    net = tf.reduce_mean(net, [1,2], name='global_pool', keep_dims=True)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block{}: {}'.format(len(block_layers)+1, net.shape))  \n",
    "                with tf.variable_scope('classification'):\n",
    "                    net = slim.flatten(net, scope='flattern')\n",
    "                    net = slim.fully_connected(net, num_classes, activation_fn=None, normalizer_fn=None,\n",
    "                                               scope='logit')\n",
    "                    if verbose:\n",
    "                        print('classification: {}'.format(net.shape))\n",
    "                    return net\n",
    "                    \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (64, 24, 24, 3)\n",
      "block0: (64, 5, 5, 64)\n",
      "block1: (64, 2, 2, 128)\n",
      "WARNING:tensorflow:From <ipython-input-5-88413f15f320>:28: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "block5: (64, 1, 1, 128)\n",
      "classification: (64, 10)\n",
      "WARNING:tensorflow:From <ipython-input-6-1dcec029f84d>:28: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step1  train loss 2.613547 train acc: 0.187500\n",
      "Step11  train loss 2.326733 train acc: 0.218750\n",
      "Step21  train loss 2.337852 train acc: 0.140625\n",
      "Step31  train loss 2.134678 train acc: 0.250000\n",
      "Step41  train loss 1.977512 train acc: 0.406250\n",
      "Step51  train loss 2.132381 train acc: 0.265625\n",
      "Step61  train loss 1.976288 train acc: 0.265625\n",
      "Step71  train loss 2.107504 train acc: 0.203125\n",
      "Step81  train loss 1.965948 train acc: 0.343750\n",
      "Step91  train loss 1.960451 train acc: 0.359375\n",
      "Step101  train loss 1.824190 train acc: 0.406250\n",
      "Step111  train loss 1.935184 train acc: 0.296875\n",
      "Step121  train loss 1.747360 train acc: 0.359375\n",
      "Step131  train loss 1.815198 train acc: 0.421875\n",
      "Step141  train loss 2.040463 train acc: 0.343750\n",
      "Step151  train loss 1.976556 train acc: 0.375000\n",
      "Step161  train loss 1.873326 train acc: 0.328125\n",
      "Step171  train loss 1.837142 train acc: 0.218750\n",
      "Step181  train loss 1.807585 train acc: 0.265625\n",
      "Step191  train loss 1.847937 train acc: 0.312500\n",
      "Step201  train loss 1.832874 train acc: 0.359375\n",
      "Step211  train loss 1.775000 train acc: 0.375000\n",
      "Step221  train loss 1.854587 train acc: 0.328125\n",
      "Step231  train loss 1.809900 train acc: 0.406250\n",
      "Step241  train loss 1.899391 train acc: 0.312500\n",
      "Step251  train loss 1.638879 train acc: 0.437500\n",
      "Step261  train loss 1.852195 train acc: 0.359375\n",
      "Step271  train loss 1.840347 train acc: 0.421875\n",
      "Step281  train loss 1.884126 train acc: 0.312500\n",
      "Step291  train loss 1.865197 train acc: 0.312500\n",
      "Step301  train loss 1.815281 train acc: 0.343750\n",
      "Step311  train loss 1.742201 train acc: 0.328125\n",
      "Step321  train loss 1.698719 train acc: 0.406250\n",
      "Step331  train loss 1.805150 train acc: 0.328125\n",
      "Step341  train loss 1.750685 train acc: 0.281250\n",
      "Step351  train loss 1.734593 train acc: 0.375000\n",
      "Step361  train loss 1.637566 train acc: 0.437500\n",
      "Step371  train loss 1.551348 train acc: 0.437500\n",
      "Step381  train loss 1.603683 train acc: 0.375000\n",
      "Step391  train loss 1.983381 train acc: 0.312500\n",
      "Step401  train loss 1.500838 train acc: 0.468750\n",
      "Step411  train loss 1.623112 train acc: 0.453125\n",
      "Step421  train loss 1.689862 train acc: 0.390625\n",
      "Step431  train loss 1.582094 train acc: 0.468750\n",
      "Step441  train loss 1.595557 train acc: 0.515625\n",
      "Step451  train loss 1.530838 train acc: 0.453125\n",
      "Step461  train loss 1.682138 train acc: 0.375000\n",
      "Step471  train loss 1.730997 train acc: 0.343750\n",
      "Step481  train loss 1.661762 train acc: 0.437500\n",
      "Step491  train loss 1.637651 train acc: 0.343750\n",
      "Step501  train loss 1.576528 train acc: 0.390625\n",
      "Step511  train loss 1.782002 train acc: 0.390625\n",
      "Step521  train loss 1.635327 train acc: 0.390625\n",
      "Step531  train loss 1.530272 train acc: 0.437500\n",
      "Step541  train loss 1.592992 train acc: 0.531250\n",
      "Step551  train loss 1.670926 train acc: 0.437500\n",
      "Step561  train loss 1.651183 train acc: 0.453125\n",
      "Step571  train loss 1.681044 train acc: 0.453125\n",
      "Step581  train loss 1.570304 train acc: 0.500000\n",
      "Step591  train loss 1.480128 train acc: 0.546875\n",
      "Step601  train loss 1.547146 train acc: 0.468750\n",
      "Step611  train loss 1.625003 train acc: 0.500000\n",
      "Step621  train loss 1.592780 train acc: 0.468750\n",
      "Step631  train loss 1.640895 train acc: 0.406250\n",
      "Step641  train loss 1.438902 train acc: 0.562500\n",
      "Step651  train loss 1.560487 train acc: 0.484375\n",
      "Step661  train loss 1.520499 train acc: 0.515625\n",
      "Step671  train loss 1.542170 train acc: 0.312500\n",
      "Step681  train loss 1.598859 train acc: 0.453125\n",
      "Step691  train loss 1.499884 train acc: 0.421875\n",
      "Step701  train loss 1.429235 train acc: 0.484375\n",
      "Step711  train loss 1.539523 train acc: 0.453125\n",
      "Step721  train loss 1.605333 train acc: 0.421875\n",
      "Step731  train loss 1.474588 train acc: 0.484375\n",
      "Step741  train loss 1.420514 train acc: 0.515625\n",
      "Step751  train loss 1.636671 train acc: 0.421875\n",
      "Step761  train loss 1.532572 train acc: 0.468750\n",
      "Step771  train loss 1.390341 train acc: 0.468750\n",
      "Step781  train loss 1.530264 train acc: 0.359375\n",
      "Step791  train loss 1.492575 train acc: 0.421875\n",
      "Step801  train loss 1.746784 train acc: 0.359375\n",
      "Step811  train loss 1.633054 train acc: 0.390625\n",
      "Step821  train loss 1.467445 train acc: 0.421875\n",
      "Step831  train loss 1.314003 train acc: 0.562500\n",
      "Step841  train loss 1.543509 train acc: 0.406250\n",
      "Step851  train loss 1.425839 train acc: 0.578125\n",
      "Step861  train loss 1.584908 train acc: 0.437500\n",
      "Step871  train loss 1.521396 train acc: 0.515625\n",
      "Step881  train loss 1.578266 train acc: 0.515625\n",
      "Step891  train loss 1.393324 train acc: 0.515625\n",
      "Step901  train loss 1.513846 train acc: 0.515625\n",
      "Step911  train loss 1.618191 train acc: 0.406250\n",
      "Step921  train loss 1.604906 train acc: 0.359375\n",
      "Step931  train loss 1.467009 train acc: 0.468750\n",
      "Step941  train loss 1.584332 train acc: 0.390625\n",
      "Step951  train loss 1.406040 train acc: 0.515625\n",
      "Step961  train loss 1.577075 train acc: 0.421875\n",
      "Step971  train loss 1.334462 train acc: 0.468750\n",
      "Step981  train loss 1.650307 train acc: 0.437500\n",
      "Step991  train loss 1.434884 train acc: 0.500000\n"
     ]
    }
   ],
   "source": [
    "with slim.arg_scope([slim.conv2d], activation_fn = tf.nn.relu, normalizer_fn = slim.batch_norm) as sc:\n",
    "    conv_scope = sc\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "image_holder = tf.placeholder(shape=[batch_size,24,24,3], dtype=tf.float32)\n",
    "label_holder = tf.placeholder(shape=[batch_size], dtype=tf.int32)\n",
    "\n",
    "with slim.arg_scope(conv_scope):\n",
    "    train_out = densenet(image_holder,10, is_training=is_training, verbose=True)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=train_out,\n",
    "                                                                       labels=label_holder, scope='train'))\n",
    "\n",
    "with tf.variable_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1,output_type=tf.int32), \n",
    "                                                    label_holder), tf.float32))\n",
    "\n",
    "lr = 1e-3\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners()\n",
    "\n",
    "for i in range(1000):\n",
    "    image_bath, label_batch = sess.run([images_trains, labels_trains])\n",
    "    _, loss1, acc1 = sess.run([train_op,train_loss,train_acc], feed_dict={image_holder: image_bath, label_holder: label_batch, is_training:True})\n",
    "    if i % 10 ==0:\n",
    "        print(\"Step%d  train loss %.6f train acc: %.6f\" % (i+1, loss1, acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
