{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /media/liangsh/新加卷1/天池阿里云/天池课程练习tf/卷积神经网络/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import  absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import  print_function\n",
    "\n",
    "import cifar10_input\n",
    "\n",
    "data_dir = \"../cifar_data/cifar-10-batches-bin/\"\n",
    "batch_size = 64\n",
    "\n",
    "train_imgs, train_labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "\n",
    "val_images, val_labels = cifar10_input.inputs(eval_data=True,data_dir=\"../cifar_data/cifar-10-batches-bin/\",\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "train_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "val_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_weight(shape, stddev=5e-2):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='weight')\n",
    "\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='bias')\n",
    "\n",
    "def conv(x, ksize, out_depth, strides, padding='SAME', act=tf.nn.relu, scope='conv_layer', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = ksize+[in_depth, out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(shape=shape)\n",
    "        strides = [1, strides[0], strides[1],1]\n",
    "        conv = tf.nn.conv2d(x, kernel, strides, padding, name='conv')\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        preact = tf.nn.bias_add(conv,bias)\n",
    "        out = act(preact)\n",
    "        return out\n",
    "\n",
    "def max_pool(x, ksize, strides, padding='SAME', name='pool_layer'):\n",
    "    return tf.nn.max_pool(x,[1, ksize[0], ksize[1],1],[1,strides[0], strides[1],1], padding, name=name)\n",
    "\n",
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with tf.variable_scope('weight'):\n",
    "            weitht = variable_weight([in_depth, out_depth])\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias= variable_bias([out_depth])\n",
    "        fc = tf.nn.bias_add(tf.matmul(x,weitht),bias, name='fc')\n",
    "        out = act(fc)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def alexnet(inputs, reuse=None):\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        #第一层， 5x5卷积， 64个卷积核， 步长1x1， padding 'VALID'   24*24\n",
    "        net = conv(inputs, [5,5], 64, [1,1], padding=\"VALID\", scope='conv1')\n",
    "        # 第二层是 3x3 的池化, 步长是 2x2, padding是`VALID`    20*20\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool1')\n",
    "        #第三层                 10*10\n",
    "        net = conv(net, [5,5], 64, [1,1], scope='conv2')\n",
    "        #第四层         10*10\n",
    "        net = max_pool(net, [3,3], [2,2], padding=\"VALID\", name='pool2')\n",
    "        \n",
    "        #return net.get_shape()\n",
    "        net = tf.reshape(net, [-1, 4*4*64])\n",
    "        net = fc(net, 384, scope='fc3')\n",
    "        net = fc(net, 192, scope='fc4')\n",
    "        net = fc(net, 10, scope='fc5', act=tf.identity)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#定义输入数据的　placeholder\n",
    "image_holder = tf.placeholder(tf.float32,[batch_size,24,24,3])\n",
    "label_holder = tf.placeholder(tf.int32,[batch_size])\n",
    "\n",
    "train_out = alexnet(image_holder)\n",
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels,tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, \n",
    "                                                                name='cross_entropy_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses',cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'),name='total_loss')\n",
    "loss = loss(train_out, label_holder)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "#top_k_op = tf.nn.in_top_k(train_out, label_holder,1)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "tf.train.start_queue_runners()\n",
    "\n",
    "for step in range(1000):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([train_imgs, train_labels])\n",
    "    _, loss_value = sess.run([train_op, loss],feed_dict={image_holder: image_batch, label_holder: label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 ==0:\n",
    "        examples_per_sec = batch_size/duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str = ('step %d, loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-294b01a80ae7>:21: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 100 trainloss: 0.281250 trainacc: 1.787735 val_loss: 1.814496 val_acc: 0.375000\n",
      "Step 500 trainloss: 0.359375 trainacc: 1.671352 val_loss: 1.717936 val_acc: 0.453125\n"
     ]
    }
   ],
   "source": [
    "train_out = alexnet(train_imgs)\n",
    "val_out = alexnet(val_images,reuse=True)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=train_out, labels=train_labels, scope='train'))\n",
    "    val_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=val_out, labels=val_labels, scope='val'))\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32),\n",
    "                                                    train_labels), tf.float32))\n",
    "    with tf.name_scope('train'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32),\n",
    "                                                  val_labels), tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners()\n",
    "\n",
    "for i in range(20000):\n",
    "    sess.run([train_imgs, train_labels])\n",
    "    sess.run(train_op)\n",
    "    if i % 400 == 99:\n",
    "        sess.run([val_images,val_labels])\n",
    "        train_acct, train_losst, val_losst, val_acct= sess.run([train_acc,train_loss,val_loss, val_acc])\n",
    "        #print(train_acct, train_losst)\n",
    "        print(\"Step {} trainloss: {:.6f} trainacc: {:.6f} val_loss: {:.6f} val_acc: {:.6f}\".format(i+1,train_acct, train_losst, val_losst, val_acct))\n",
    "    \n",
    "print(\"train done !\")\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
