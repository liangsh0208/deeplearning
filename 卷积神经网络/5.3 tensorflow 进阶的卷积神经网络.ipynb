{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/liangsh/Elements/深度学习/03.tensorflow/5.3 tensorflow进阶的卷积神经网络/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /media/liangsh/Elements/深度学习/03.tensorflow/5.3 tensorflow进阶的卷积神经网络/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "WARNING:tensorflow:From /media/liangsh/Elements/深度学习/03.tensorflow/5.3 tensorflow进阶的卷积神经网络/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /media/liangsh/Elements/深度学习/03.tensorflow/5.3 tensorflow进阶的卷积神经网络/cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-1-0b5efee8ed80>:80: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, loss=4.68 (8.4 examples/sec; 15.172 sec/batch)\n",
      "step 10, loss=3.78 (785.4 examples/sec; 0.163 sec/batch)\n",
      "step 20, loss=3.13 (907.7 examples/sec; 0.141 sec/batch)\n",
      "step 30, loss=2.72 (695.8 examples/sec; 0.184 sec/batch)\n",
      "step 40, loss=2.43 (897.1 examples/sec; 0.143 sec/batch)\n",
      "step 50, loss=2.24 (980.8 examples/sec; 0.131 sec/batch)\n",
      "step 60, loss=2.17 (1051.9 examples/sec; 0.122 sec/batch)\n",
      "step 70, loss=2.07 (927.8 examples/sec; 0.138 sec/batch)\n",
      "step 80, loss=2.04 (1009.9 examples/sec; 0.127 sec/batch)\n",
      "step 90, loss=1.97 (1011.1 examples/sec; 0.127 sec/batch)\n",
      "step 100, loss=1.94 (930.1 examples/sec; 0.138 sec/batch)\n",
      "step 110, loss=1.93 (902.0 examples/sec; 0.142 sec/batch)\n",
      "step 120, loss=1.93 (1063.5 examples/sec; 0.120 sec/batch)\n",
      "step 130, loss=2.02 (960.5 examples/sec; 0.133 sec/batch)\n",
      "step 140, loss=1.91 (969.4 examples/sec; 0.132 sec/batch)\n",
      "step 150, loss=1.70 (980.1 examples/sec; 0.131 sec/batch)\n",
      "step 160, loss=1.79 (1044.4 examples/sec; 0.123 sec/batch)\n",
      "step 170, loss=1.80 (985.3 examples/sec; 0.130 sec/batch)\n",
      "step 180, loss=1.67 (1017.0 examples/sec; 0.126 sec/batch)\n",
      "step 190, loss=1.72 (1027.7 examples/sec; 0.125 sec/batch)\n",
      "step 200, loss=1.79 (910.7 examples/sec; 0.141 sec/batch)\n",
      "step 210, loss=1.65 (1063.6 examples/sec; 0.120 sec/batch)\n",
      "step 220, loss=1.66 (1030.6 examples/sec; 0.124 sec/batch)\n",
      "step 230, loss=1.63 (888.8 examples/sec; 0.144 sec/batch)\n",
      "step 240, loss=1.64 (976.4 examples/sec; 0.131 sec/batch)\n",
      "step 250, loss=1.89 (1030.7 examples/sec; 0.124 sec/batch)\n",
      "step 260, loss=1.72 (933.8 examples/sec; 0.137 sec/batch)\n",
      "step 270, loss=1.56 (978.4 examples/sec; 0.131 sec/batch)\n",
      "step 280, loss=1.69 (1021.7 examples/sec; 0.125 sec/batch)\n",
      "step 290, loss=1.47 (988.8 examples/sec; 0.129 sec/batch)\n",
      "step 300, loss=1.49 (963.6 examples/sec; 0.133 sec/batch)\n",
      "step 310, loss=1.50 (1032.5 examples/sec; 0.124 sec/batch)\n",
      "step 320, loss=1.67 (1002.5 examples/sec; 0.128 sec/batch)\n",
      "step 330, loss=1.49 (859.8 examples/sec; 0.149 sec/batch)\n",
      "step 340, loss=1.65 (1050.8 examples/sec; 0.122 sec/batch)\n",
      "step 350, loss=1.46 (995.4 examples/sec; 0.129 sec/batch)\n",
      "step 360, loss=1.36 (939.9 examples/sec; 0.136 sec/batch)\n",
      "step 370, loss=1.43 (992.6 examples/sec; 0.129 sec/batch)\n",
      "step 380, loss=1.61 (1030.5 examples/sec; 0.124 sec/batch)\n",
      "step 390, loss=1.54 (538.1 examples/sec; 0.238 sec/batch)\n",
      "step 400, loss=1.49 (933.0 examples/sec; 0.137 sec/batch)\n",
      "step 410, loss=1.46 (1022.8 examples/sec; 0.125 sec/batch)\n",
      "step 420, loss=1.70 (992.7 examples/sec; 0.129 sec/batch)\n",
      "step 430, loss=1.47 (923.2 examples/sec; 0.139 sec/batch)\n",
      "step 440, loss=1.40 (1043.6 examples/sec; 0.123 sec/batch)\n",
      "step 450, loss=1.68 (954.9 examples/sec; 0.134 sec/batch)\n",
      "step 460, loss=1.65 (976.8 examples/sec; 0.131 sec/batch)\n",
      "step 470, loss=1.58 (1034.5 examples/sec; 0.124 sec/batch)\n",
      "step 480, loss=1.60 (1016.5 examples/sec; 0.126 sec/batch)\n",
      "step 490, loss=1.46 (837.2 examples/sec; 0.153 sec/batch)\n",
      "step 500, loss=1.21 (961.3 examples/sec; 0.133 sec/batch)\n",
      "step 510, loss=1.49 (984.8 examples/sec; 0.130 sec/batch)\n",
      "step 520, loss=1.59 (896.0 examples/sec; 0.143 sec/batch)\n",
      "step 530, loss=1.36 (853.2 examples/sec; 0.150 sec/batch)\n",
      "step 540, loss=1.47 (1026.1 examples/sec; 0.125 sec/batch)\n",
      "step 550, loss=1.30 (1015.0 examples/sec; 0.126 sec/batch)\n",
      "step 560, loss=1.49 (873.2 examples/sec; 0.147 sec/batch)\n",
      "step 570, loss=1.36 (973.5 examples/sec; 0.131 sec/batch)\n",
      "step 580, loss=1.50 (970.5 examples/sec; 0.132 sec/batch)\n",
      "step 590, loss=1.36 (990.3 examples/sec; 0.129 sec/batch)\n",
      "step 600, loss=1.58 (796.6 examples/sec; 0.161 sec/batch)\n",
      "step 610, loss=1.43 (999.3 examples/sec; 0.128 sec/batch)\n",
      "step 620, loss=1.41 (894.9 examples/sec; 0.143 sec/batch)\n",
      "step 630, loss=1.46 (857.0 examples/sec; 0.149 sec/batch)\n",
      "step 640, loss=1.39 (1025.0 examples/sec; 0.125 sec/batch)\n",
      "step 650, loss=1.32 (997.9 examples/sec; 0.128 sec/batch)\n",
      "step 660, loss=1.34 (934.9 examples/sec; 0.137 sec/batch)\n",
      "step 670, loss=1.43 (1054.7 examples/sec; 0.121 sec/batch)\n",
      "step 680, loss=1.31 (1019.0 examples/sec; 0.126 sec/batch)\n",
      "step 690, loss=1.31 (980.7 examples/sec; 0.131 sec/batch)\n",
      "step 700, loss=1.21 (998.9 examples/sec; 0.128 sec/batch)\n",
      "step 710, loss=1.28 (1020.7 examples/sec; 0.125 sec/batch)\n",
      "step 720, loss=1.23 (923.8 examples/sec; 0.139 sec/batch)\n",
      "step 730, loss=1.34 (901.7 examples/sec; 0.142 sec/batch)\n",
      "step 740, loss=1.29 (1040.1 examples/sec; 0.123 sec/batch)\n",
      "step 750, loss=1.45 (961.8 examples/sec; 0.133 sec/batch)\n",
      "step 760, loss=1.57 (972.2 examples/sec; 0.132 sec/batch)\n",
      "step 770, loss=1.18 (1005.4 examples/sec; 0.127 sec/batch)\n",
      "step 780, loss=1.25 (946.9 examples/sec; 0.135 sec/batch)\n",
      "step 790, loss=1.31 (871.0 examples/sec; 0.147 sec/batch)\n",
      "step 800, loss=1.38 (986.0 examples/sec; 0.130 sec/batch)\n",
      "step 810, loss=1.22 (1000.7 examples/sec; 0.128 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 820, loss=1.21 (868.0 examples/sec; 0.147 sec/batch)\n",
      "step 830, loss=1.47 (1027.4 examples/sec; 0.125 sec/batch)\n",
      "step 840, loss=1.32 (1058.2 examples/sec; 0.121 sec/batch)\n",
      "step 850, loss=1.29 (958.2 examples/sec; 0.134 sec/batch)\n",
      "step 860, loss=1.37 (987.0 examples/sec; 0.130 sec/batch)\n",
      "step 870, loss=1.55 (1022.3 examples/sec; 0.125 sec/batch)\n",
      "step 880, loss=1.36 (842.3 examples/sec; 0.152 sec/batch)\n",
      "step 890, loss=1.37 (1014.3 examples/sec; 0.126 sec/batch)\n",
      "step 900, loss=1.25 (1006.5 examples/sec; 0.127 sec/batch)\n",
      "step 910, loss=1.30 (938.9 examples/sec; 0.136 sec/batch)\n",
      "step 920, loss=1.37 (966.2 examples/sec; 0.132 sec/batch)\n",
      "step 930, loss=1.15 (1042.6 examples/sec; 0.123 sec/batch)\n",
      "step 940, loss=1.21 (877.6 examples/sec; 0.146 sec/batch)\n",
      "step 950, loss=1.23 (698.2 examples/sec; 0.183 sec/batch)\n",
      "step 960, loss=1.25 (1120.1 examples/sec; 0.114 sec/batch)\n",
      "step 970, loss=1.13 (951.3 examples/sec; 0.135 sec/batch)\n",
      "step 980, loss=1.28 (928.5 examples/sec; 0.138 sec/batch)\n",
      "step 990, loss=1.30 (1016.0 examples/sec; 0.126 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10_input\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "max_steps = 300\n",
    "batch_size = 128\n",
    "\n",
    "#定义截断正太分布，初始化权重，并添加Ｌ2正则化项\n",
    "def variable_with_weight_loss(shape,stddev,wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape,stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),wl,name='weight_loss')\n",
    "        tf.add_to_collection('losses',weight_loss)\n",
    "    return var\n",
    "\n",
    "images_trains, labels_trains = cifar10_input.distorted_inputs(data_dir=\"./cifar_data/cifar-10-batches-bin/\",\n",
    "                                                              batch_size=batch_size)\n",
    "\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True,data_dir=\"./cifar_data/cifar-10-batches-bin/\",\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "#定义输入数据的　placeholder\n",
    "image_holder = tf.placeholder(tf.float32,[batch_size,24,24,3])\n",
    "label_holder = tf.placeholder(tf.int32,[batch_size])\n",
    "\n",
    "#定义第一个卷积层\n",
    "weight1 = variable_with_weight_loss(shape=[5,5,3,64],stddev=5e-2,wl=0.0)\n",
    "kernerl1 =tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding=\"SAME\")\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernerl1,bias1))\n",
    "pool1 = tf.nn.max_pool(conv1,ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\")\n",
    "norm1 = tf.nn.lrn(pool1, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "#第二个卷积层\n",
    "weight2 = variable_with_weight_loss(shape=[5,5,64,64],stddev=5e-2, wl=0.0)\n",
    "kernerl2 = tf.nn.conv2d(norm1, weight2, [1,1,1,1], padding=\"SAME\")\n",
    "bias2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernerl2,bias2))\n",
    "norm2 = tf.nn.lrn(conv2,bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2,ksize=[1,3,3,1],strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "#全连接层\n",
    "reshape = tf.reshape(pool2,[batch_size,-1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim,384],stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bias3)\n",
    "\n",
    "#第二个全连接层\n",
    "weight4 = variable_with_weight_loss(shape=[384,192], stddev=0.04, wl = 0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1,shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3,weight4)+bias4)\n",
    "\n",
    "#最后一层\n",
    "weight5 = variable_with_weight_loss(shape=[192,10],stddev=1/192.0, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4,weight5),bias5)\n",
    "\n",
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels,tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, \n",
    "                                                                   name='cross_entropy_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses',cross_entropy_mean)\n",
    "    \n",
    "    return tf.add_n(tf.get_collection('losses'),name='total_loss')\n",
    "\n",
    "loss = loss(logits, label_holder)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder,1)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "tf.train.start_queue_runners()\n",
    "\n",
    "for step in range(1000):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([images_trains, labels_trains])\n",
    "    _, loss_value = sess.run([train_op, loss],feed_dict={image_holder: image_batch, label_holder: label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 ==0:\n",
    "        examples_per_sec = batch_size/duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str = ('step %d, loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples/batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    prediction = sess.run([top_k_op], feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    \n",
    "    true_count += np.sum(prediction)\n",
    "    step +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.632\n"
     ]
    }
   ],
   "source": [
    "precision = true_count/ total_sample_count\n",
    "print('precision = %.3f' %precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
