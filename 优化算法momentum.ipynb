{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5f635d2ec5ae>:14: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division\n",
    "from __future__ import  absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "import sys\n",
    "\n",
    "tf.set_random_seed(2019)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test\n",
    "\n",
    "def hidden_layer(layer_input, output_depth, scope = \"hidden_layer\", reuse=None,weights_collection=None, \n",
    "                 biases_collection=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                            shape=(input_depth, output_depth),name=\"weights\")\n",
    "        tf.add_to_collection(weights_collection,w)\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name=\"bias\")\n",
    "        tf.add_to_collection(biases_collection,b)\n",
    "        net = tf.matmul(layer_input,w)+b\n",
    "        \n",
    "        return net\n",
    "\n",
    "def DNN(x, output_depths, scope='DNN', reuse=None,weights_collection=None, biases_collection=None):\n",
    "    net=x\n",
    "    for i , output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net,output_depth,scope='layer%d' % i, reuse=reuse,\n",
    "                           weights_collection=weights_collection, biases_collection=biases_collection)\n",
    "        net = tf.nn.relu(net)\n",
    "    net = hidden_layer(net,10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net\n",
    "\n",
    "input_ph = tf.placeholder(shape=(None,784),dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None,10), dtype=tf.int64)\n",
    "\n",
    "dnn = DNN(input_ph, [400,200,100],weights_collection='params', biases_collection='params')\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn,axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "\n",
    "params = tf.get_collection('params')\n",
    "gradients = tf.gradients(loss,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(params, gradients, vs, lr, gamma, name='momentum_update'):\n",
    "    update_ops = []\n",
    "    for param, gradient, v in zip(params,gradients, vs):\n",
    "        v_update = v.assign(gamma*v+ lr*gradient)\n",
    "        with tf.control_dependencies([v_update]):\n",
    "            update_ops.append(param.assign_sub(v))\n",
    "    update_op = tf.group(*update_ops, name=name)\n",
    "    return update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('moments'):\n",
    "    for i, param in enumerate(params):\n",
    "        v = tf.get_variable(param.op.name, shape=param.get_shape(),\n",
    "                            initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "        tf.add_to_collection('moments',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = tf.get_collection('moments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_op = momentum_update(params, gradients, vs, 0.01, 0.9)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Trian loss: 0.193582 Accuracy: 0.943800\n",
      "Epoch2: Trian loss: 0.136190 Accuracy: 0.959055\n",
      "Epoch3: Trian loss: 0.099330 Accuracy: 0.970709\n",
      "Epoch4: Trian loss: 0.071802 Accuracy: 0.979691\n",
      "Epoch5: Trian loss: 0.064468 Accuracy: 0.981218\n",
      "Train done ! Cost Time: 17.80\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "sample_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "while(epoch < 5):\n",
    "    if sample_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - sample_passed\n",
    "        sample_passed =0\n",
    "        epoch +=1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        sample_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    \n",
    "    if epoch_done:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train, acc_train = sess.run([loss,acc], feed_dict={input_ph: image, label_ph: label})\n",
    "            #print(acc_train)\n",
    "            train_acc.append(acc_train)\n",
    "            train_loss.append(loss_train)\n",
    "        #print(train_acc)\n",
    "        print('Epoch{}: Trian loss: {:.6f} Accuracy: {:.6f}'.format(epoch, np.array(train_loss).mean(),\n",
    "                                                                    np.array(train_acc).mean()))\n",
    "        \n",
    "        epoch_done = False\n",
    "    \n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step +=1\n",
    "_end = time.time()\n",
    "print(\"Train done ! Cost Time: {:.2f}\".format(_end - _start))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Trian loss: 0.140403 Accuracy: 0.959891\n",
      "Epoch2: Trian loss: 0.090328 Accuracy: 0.972236\n",
      "Epoch3: Trian loss: 0.059697 Accuracy: 0.982800\n",
      "Epoch4: Trian loss: 0.041807 Accuracy: 0.987327\n",
      "Epoch5: Trian loss: 0.032857 Accuracy: 0.989945\n",
      "Train done ! Cost Time: 19.53\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 64\n",
    "train_losses2 = []\n",
    "\n",
    "epoch = 0\n",
    "sample_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "while(epoch < 5):\n",
    "    if sample_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - sample_passed\n",
    "        sample_passed =0\n",
    "        epoch +=1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        sample_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    \n",
    "    if epoch_done:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train, acc_train = sess.run([loss,acc], feed_dict={input_ph: image, label_ph: label})\n",
    "            #print(acc_train)\n",
    "            train_acc.append(acc_train)\n",
    "            train_loss.append(loss_train)\n",
    "        #print(train_acc)\n",
    "        print('Epoch{}: Trian loss: {:.6f} Accuracy: {:.6f}'.format(epoch, np.array(train_loss).mean(),\n",
    "                                                                    np.array(train_acc).mean()))\n",
    "        \n",
    "        epoch_done = False\n",
    "    \n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses2.append(loss_train)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step +=1\n",
    "_end = time.time()\n",
    "print(\"Train done ! Cost Time: {:.2f}\".format(_end - _start))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
